#TODO: Load model, evaluate token and sequence level repitition, rerun training to get training/validation loss and perplexity

#Experiment 1: Measure of repeating sequences generated by model with no prefixes from validation set
#Experiment 2: Measure of repeating sequences generated by the model with prefixes from validation set
#Experiement 3: Next-token level accuracy of the model given sequences from validation set
#Experiment 4: Measure of repeating tokens generated by the model on sequences from validation set

#TODO: Load model, evaluate token and sequence level repitition, rerun training to get training/validation loss and perplexity

#Experiment 1: Measure of repeating sequences generated by model with no prefixes from validation set
#Experiment 2: Measure of repeating sequences generated by the model with prefixes from validation set
#Experiement 3: Next-token level accuracy of the model given sequences from validation set
#Experiment 4: Measure of repeating tokens generated by the model on sequences from validation set

from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch
import random
import numpy as np
import pandas as pd
import os
from google.colab import drive

torch.manual_seed(42)
random.seed(42)
np.random.seed(42)

#Load a trained model that we fine-tuned and its vocabulary
def load_model(path, device):
    model = GPT2LMHeadModel.from_pretrained(path)
    tokenizer = GPT2Tokenizer.from_pretrained(path)
    model.to(device)
    return model,tokenizer

#Calculate the portion of duplicated n-grams in a generated sequence
#Note: This funciton assumes the sentence length > n_gram length

#Formula: seq-rep-n = 1 - |unique n-gram| / |n-grams|
#Where the bars indicate magnitude/size.
#So its 1- (the number of unique n-grams generated divided by the total number of ngrams)

def sequence_level_rep(sentence, n_gram_length):
    sequences = []

    for i in range(0, len(sentence.split())-n_gram_length+1):
        # select sequence of tokens
        seq = sentence.split()[i:i+n_gram_length]
        # add to the list
        sequences.append(" ".join(seq))
    dict_of_counts = {n_gram: sequences.count(n_gram) for n_gram in sequences}

    unique_n_gram_count = sum(i for i in dict_of_counts.values() if i == 1)
    #print('Unique n-grams: ' ,unique_n_gram_count)

    if unique_n_gram_count == 0:
      return 0

    sequence_repitition_proportion = 1 - (unique_n_gram_count/sum(dict_of_counts.values()))

    #print('N-grams: ', dict_of_counts)
    return sequence_repitition_proportion

#Calculate the proportion of duplicated tokens in a generated sequence

#Input: A set, D, of length T sequences
#Goal: Loop through all sequences, and for the previous l tokens if the top-1 token to be predicted is either of those, increment by one

def token_level_rep(model, D, tokenizer, device):
    pass

drive.mount('/gdrive', force_remount=True)

gdrive_dir = '/content/gdrive/'
#%cd content/drive/My Drive/models/
directory= '/gdrive/My Drive/models/T_Loss_0.198_V_Loss_0.208'

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

model, tokenizer = load_model(directory, device)
val = pd.read_csv('data/clean_poems_test.csv')
val_df = pd.read_csv('/gdrive/My Drive/models/clean_poems_test.csv')



N = 100
k = 50
p = 0.95

#Take out N poems and perform experiments 2, 3, and 4
val_df.sample(n=N)

#Need to generate a poem strictly that of gold text's size
def generate_exp_2(model, tokenizer, text, device, gold_sentence_len):
  generated = torch.tensor(tokenizer.encode(text)).unsqueeze(0)
  generated = generated.to(device)

  sample_outputs = model.generate(
      generated,
      # bos_token_id=random.randint(1,30000),
      do_sample=True,
      top_k=50,
      # the K most likely next words are filtered and the probability mass is redistributed among only those K next words.
      max_length=gold_sentence_len,
      min_length=gold_sentence_len,
      top_p=0.95  # Top-p sampling picks the minimum number of words to exceed together p=[]%
      # num_return_sequences=4  #Uncomment this for multiple, independently sampled outputs
  )
  return tokenizer.decode(sample_outputs[0], skip_special_tokens=True)

#Experiment 2: Measure of repeating sequences generated by the model with prefixes from validation set
def experiment_2(N, model, D, tokenizer, prefix_len, n_gram_length):

  #Loop through the list of sentences and create a list of predictions for each sentence
  #THen send each of those to the sequence_level_rep function
  proportion_sum = 0
  for sentence in D:
    sentence = sentence.split(' ')
    prefix = sentence[:prefix_len] #Prefix is the first [prefix_len] words
    sentence_len = len(sentence)


    text = " ".join(prefix) + ""
    generated_text = generate_exp_2(model, tokenizer, text, device, sentence_len)
    #print(generated_text)
    #print(prefix)
    #print(len(sentence), len(generated_text.split(' ')))
    #print('Gold: ', sentence)

    proportion_sum += sequence_level_rep(generated_text, n_gram_length)
  return proportion_sum/N

#measure_2 = experiment_2(N, model, val_df.Poem.to_list(), tokenizer, 4, 4)
#print('Proportion of repeating sequences with prefixes from validation set: ', measure_2)


def generate_exp_1(N, model, tokenizer, device, prompt="<|startoftext|>"):
    generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)
    generated = generated.to(device)
    sample_outputs = model.generate(
                                    generated,
                                    #bos_token_id=random.randint(1,30000),
                                    do_sample=True,
                                    top_k=50, #the K most likely next words are filtered and the probability mass is redistributed among only those K next words.
                                    max_length = 60,  #15 max words * 4 number of lines
                                    min_length = 12, #3 words minimum * 4 number of lines
                                    top_p=0.95, #Top-p sampling picks the minimum number of words to exceed together p=[]%
                                    num_return_sequences=N  #Uncomment this for multiple, independently sampled outputs
                                    )
    outputs = [tokenizer.decode(sample_outputs[i], skip_special_token=True) for i in range(N)]
    return outputs


#Experiment 1: Measure of repeating sequences generated by model with no prefixes from validation set
def experiment_1(N, model, tokenizer, n_gram_length, device):
    generated_texts = generate_exp_1(N, model, tokenizer, device)
    proportion_sum = 0

    for i in generated_texts:
      proportion_sum += sequence_level_rep(i, n_gram_length)
    return proportion_sum/N

#measure_1 = experiment_1(N, model, tokenizer, 4, device)
#print('Proportion of repeating sequences with prefixes not from validation set: ', measure_1)



# Experiement 3: Next-token level accuracy of the model given sequences from validation set

def generate_exp_3(model, tokenizer, device, prompt="<|startoftext|>"):
    generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)
    generated = generated.to(device)

    total_tokens_so_far = generated.shape[1]
    if generated.shape[0] >= len(prompt.split()) + 1:
        print(generated, prompt, len(prompt.split()) + 1)

    sample_outputs = model.generate(
        generated,
        # bos_token_id=random.randint(1,30000),
        do_sample=True,
        pad_token_id=50256,
        top_k=50,
        # the K most likely next words are filtered and the probability mass is redistributed among only those K next words.
        max_length=total_tokens_so_far + 1,
        min_length=total_tokens_so_far + 1,
        top_p=0.95  # Top-p sampling picks the minimum number of words to exceed together p=[]%
    )
    # outputs = tokenizer.decode(sample_outputs[0], skip_special_token=True)
    outputs = tokenizer.decode(sample_outputs[0], skip_special_token=True)
    z = outputs
    outputs = outputs.replace(prompt, '')
    return prompt + ' ' + outputs
    # return outputs

def experiment_3(model, D, prefix_len):
    acc = 0
    for i, sentence in enumerate(D):
        sentence_score = 0
        sentence = sentence.split(' ')
        prefix = sentence[:prefix_len]  # Prefix is the first [prefix_len] words
        gold_tokens = sentence[prefix_len:]

        text = " ".join(prefix) + ""

        for gold_token in gold_tokens:
            if len(text) == 0: continue
            predicted_token = generate_exp_3(model, tokenizer, device, text).split(' ')[-1]
            if predicted_token == gold_token:
                sentence_score += 1
            text = " ".join([text, gold_token])
        score = sentence_score / len(sentence)
        acc += score
        print('{}/{}'.format(i, len(D)))

    return acc / len(D)


#measure_3 = experiment_3(model, val_df.Poem, 1)
#print('Proportion of repeating sequences with prefixes not from validation set: ', measure_3)


def generate_exp_4(model, tokenizer, device, prompt="<|startoftext|>"):
    generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)
    generated = generated.to(device)
    total_tokens_so_far = generated.shape[1]

    sample_outputs = model.generate(
                                    generated,
                                    #bos_token_id=random.randint(1,30000),
                                    pad_token_id=50256,
                                    do_sample=True,
                                    top_k=1, #the K most likely next words are filtered and the probability mass is redistributed among only those K next words.
                                    max_length = total_tokens_so_far+1,  #15 max words * 4 number of lines
                                    min_length = total_tokens_so_far+1, #3 words minimum * 4 number of lines
                                    top_p=0.95 #Top-p sampling picks the minimum number of words to exceed together p=[]%
                                    )
    outputs = tokenizer.decode(sample_outputs[0], skip_special_token=True)
    z = outputs
    outputs = outputs.replace(prompt, '')
    return prompt + ' ' + outputs


#Experiment 4: Measure of repeating tokens generated by the model on sequences from validation set
def experiment_4(model, D, l):
  acc = 0
  for i, sentence in enumerate(D):
    sentence_score = 0
    sentence = sentence.split(' ')

    text = ""

    for t, gold_token in enumerate(sentence, 1):
      if t == 1:
        text = gold_token
      else:
        text = " ".join([text, gold_token])
      if len(text)== 0: continue
      predicted_token = generate_exp_4(model, tokenizer, device, text).split(' ')[-1]
      previous_l_tokens = text[t - l - 1:t - 1]
      if predicted_token in previous_l_tokens:
        sentence_score += 1
    score = sentence_score / len(sentence)
    print('{}/{}'.format(i, len(D)))
    acc += score
  return acc / len(D)

measure_4 = experiment_4(model, val_df.Poem, 1)
print('Proportion of repeating sequences with prefixes not from validation set: ', measure_4)